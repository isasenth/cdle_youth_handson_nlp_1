{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14483,"status":"ok","timestamp":1653285706324,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"YKSwxGXOB6qm","outputId":"8e387f63-09b6-4cb4-e9e1-6873671b72bd"},"outputs":[],"source":["# !pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1 unidic_lite\n","\n","import random\n","import glob\n","from tqdm import tqdm\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","# MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n","# MODEL_NAME = 'cl-tohoku/bert-large-japanese'\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v2'\n","\n","# データのパス\n","PATH = ''"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653285706326,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"unmbc0kxsKkN"},"outputs":[],"source":["def make_dataset(df, max_length, tokenizer):\n","  dataset = []\n","  for index, row in df.iterrows():\n","    encoding = tokenizer(\n","        row['text'],\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True\n","    )\n","    encoding['labels']=row['isFake']\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset.append(encoding)\n","  return dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":16394,"status":"ok","timestamp":1653285722712,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"KO4S6us_vMJm"},"outputs":[],"source":["train = pd.read_csv(PATH + \"train.csv\")\n","test = pd.read_csv(PATH + \"test.csv\")\n","\n","max_length = 256\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","dataset_for_loader = make_dataset(train, max_length, tokenizer)\n","random.shuffle(dataset_for_loader)\n","n = len(dataset_for_loader)\n","n_train = int(0.7*n)\n","dataset_train = dataset_for_loader[:n_train]\n","dataset_val = dataset_for_loader[n_train:]\n","# dataset_test = make_dataset(test, max_length, tokenizer)\n","\n","dataloader_train = DataLoader(\n","    dataset_train, batch_size=32, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","# dataloader_test = DataLoader(dataset_test, batch_size=256)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1653285722713,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"VUVgf7B2wkzt"},"outputs":[],"source":["class BertForSequenceClassification_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, num_labels, lr):\n","        super().__init__()\n","        self.save_hyperparameters() \n","        self.bert_sc = BertForSequenceClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","        self.outputs = []\n","        \n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","        \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","\n","    def test_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        labels_predicted = output.logits.argmax(-1)\n","        self.outputs.extend(labels_predicted.tolist())\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1653285755351,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"It36GYKZx8t-","outputId":"50e06834-b6f7-4f57-9d7f-67caecfd1ac1"},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=10,\n","    callbacks = [checkpoint]\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":710,"referenced_widgets":["3cb4231d7c294ee0baaf77d18e6266f7","11f3b6bce2394b4ba6db8935c5e681aa","34b21915c3644513a17db6915a93753f","f30a14d421c2415c9c67827d199508a0","898218e4362342fe883b9e460f41ba96","7691d8792f7d4057ab9168c858782a3d","e7bb7ae5bd7546b99839e44eecc26063","06fedc9416764ec79afbae0b92f73f38","b4eff220e9bf4df9997aff6d41cfa8ea","3a70c7dffa4a4242945ea9f5ab9c20f9","548a044b927e42c19f8130ec28ae060b","05f300cdbd9646089c82a837b8ab38ff","43c96d07b6c64066ae2ea51be32012c3","255367ab2f8a4cd5952474cc33a8045b","e2edcc6da57042b9900e43fa7a2a048a","d5674991582e44b898a943aef47be6dc","b1eaecc4473e4ff3bb41a0218daa478a","37cb87c1fb4748dc82d50228c67e4df0","52de0568f86a48daba10ce23f11a0ff1","efc78731443742199f7bdfa0d2ebda87","f7695f2ffa00496a9ce44b37f8fd2ca0","4b71d65e426d4caf80623827bdb6ba88"]},"executionInfo":{"elapsed":25944,"status":"error","timestamp":1653285782261,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"kMTSvv6dyGau","outputId":"e1e850f2-07fb-476c-82e7-93366156a856"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 427M/427M [00:39<00:00, 11.3MB/s] \n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type                          | Params\n","----------------------------------------------------------\n","0 | bert_sc | BertForSequenceClassification | 111 M \n","----------------------------------------------------------\n","111 M     Trainable params\n","0         Non-trainable params\n","111 M     Total params\n","444.835   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 3.80 GiB total capacity; 1.54 GiB already allocated; 505.50 MiB free; 1.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000005?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification_pl(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000005?line=1'>2</a>\u001b[0m     MODEL_NAME, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000005?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000005?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dataloader_train, dataloader_val)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=804'>805</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1339'>1340</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1341'>1342</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1342'>1343</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1345'>1346</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1411\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1408'>1409</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1409'>1410</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1410'>1411</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1412'>1413</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1414'>1415</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=125'>126</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=126'>127</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=127'>128</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=129'>130</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=219'>220</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=220'>221</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=221'>222</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=223'>224</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1759'>1760</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1761'>1762</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1762'>1763</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1764'>1765</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1765'>1766</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=338'>339</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=339'>340</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=340'>341</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=341'>342</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=342'>343</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb Cell 4'\u001b[0m in \u001b[0;36mBertForSequenceClassification_pl.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000003?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000003?line=18'>19</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_sc(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000003?line=19'>20</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT_v2.ipynb#ch0000003?line=20'>21</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, val_loss)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1554\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1545'>1546</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1546'>1547</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1547'>1548</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1548'>1549</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1549'>1550</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1550'>1551</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1551'>1552</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1553'>1554</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1554'>1555</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1555'>1556</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1556'>1557</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1557'>1558</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1558'>1559</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1559'>1560</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1560'>1561</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1561'>1562</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1562'>1563</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1563'>1564</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1565'>1566</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1567'>1568</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1017\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1009'>1010</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1010'>1011</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1011'>1012</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1014'>1015</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1015'>1016</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1016'>1017</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1017'>1018</a>\u001b[0m     embedding_output,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1018'>1019</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1019'>1020</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1020'>1021</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1021'>1022</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1022'>1023</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1023'>1024</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1024'>1025</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1025'>1026</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1026'>1027</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1027'>1028</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1028'>1029</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1029'>1030</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=596'>597</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=597'>598</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=598'>599</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=602'>603</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=603'>604</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=604'>605</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=605'>606</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=606'>607</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=607'>608</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=608'>609</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=609'>610</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=610'>611</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=611'>612</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=612'>613</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=613'>614</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=615'>616</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=616'>617</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=480'>481</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=481'>482</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=482'>483</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=489'>490</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=490'>491</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=491'>492</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=492'>493</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=493'>494</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=494'>495</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=495'>496</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=496'>497</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=497'>498</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=498'>499</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=499'>500</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=501'>502</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=412'>413</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=413'>414</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=414'>415</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=420'>421</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=421'>422</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=422'>423</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=423'>424</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=427'>428</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=428'>429</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=429'>430</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=430'>431</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=431'>432</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=432'>433</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:327\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=323'>324</a>\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=325'>326</a>\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=326'>327</a>\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=328'>329</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=329'>330</a>\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 3.80 GiB total capacity; 1.54 GiB already allocated; 505.50 MiB free; 1.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["model = BertForSequenceClassification_pl(\n","    MODEL_NAME, num_labels=2, lr=1e-5\n",")\n","\n","trainer.fit(model, dataloader_train, dataloader_val) "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":382,"status":"aborted","timestamp":1653285723092,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"NuZF0m9t0dKQ"},"outputs":[],"source":["def make_dataset_test(df, max_length, tokenizer):\n","  dataset = []\n","  for index, row in df.iterrows():\n","    encoding = tokenizer(\n","        row['text'],\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True\n","    )\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset.append(encoding)\n","  return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":383,"status":"aborted","timestamp":1653285723093,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"RVM6WD18yZA-"},"outputs":[],"source":["dataset_test = make_dataset_test(test, max_length, tokenizer)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)\n","trainer.test(dataloaders=dataloader_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":384,"status":"aborted","timestamp":1653285723094,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"mUrG4m0p7dwZ"},"outputs":[],"source":["submit = pd.read_csv(PATH + \"sample_submission.csv\")\n","submit['isFake'] = model.outputs\n","submit.to_csv('/content/drive/MyDrive/FakeNewsDetection/submit.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOtHHuNnRjXCobxB12y05VK","collapsed_sections":[],"mount_file_id":"1A7sL5FlRoh26XnKfhcwIsb2Ifz_C08YF","name":"bertmodel.ipynb","provenance":[]},"kernelspec":{"display_name":"hazumi1902","language":"python","name":"hazumi1902"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05f300cdbd9646089c82a837b8ab38ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43c96d07b6c64066ae2ea51be32012c3","IPY_MODEL_255367ab2f8a4cd5952474cc33a8045b","IPY_MODEL_e2edcc6da57042b9900e43fa7a2a048a"],"layout":"IPY_MODEL_d5674991582e44b898a943aef47be6dc"}},"06fedc9416764ec79afbae0b92f73f38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f3b6bce2394b4ba6db8935c5e681aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7691d8792f7d4057ab9168c858782a3d","placeholder":"​","style":"IPY_MODEL_e7bb7ae5bd7546b99839e44eecc26063","value":"Sanity Checking DataLoader 0: 100%"}},"255367ab2f8a4cd5952474cc33a8045b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_52de0568f86a48daba10ce23f11a0ff1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efc78731443742199f7bdfa0d2ebda87","value":0}},"34b21915c3644513a17db6915a93753f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_06fedc9416764ec79afbae0b92f73f38","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4eff220e9bf4df9997aff6d41cfa8ea","value":1}},"37cb87c1fb4748dc82d50228c67e4df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a70c7dffa4a4242945ea9f5ab9c20f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb4231d7c294ee0baaf77d18e6266f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11f3b6bce2394b4ba6db8935c5e681aa","IPY_MODEL_34b21915c3644513a17db6915a93753f","IPY_MODEL_f30a14d421c2415c9c67827d199508a0"],"layout":"IPY_MODEL_898218e4362342fe883b9e460f41ba96"}},"43c96d07b6c64066ae2ea51be32012c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1eaecc4473e4ff3bb41a0218daa478a","placeholder":"​","style":"IPY_MODEL_37cb87c1fb4748dc82d50228c67e4df0","value":"Epoch 0:   0%"}},"4b71d65e426d4caf80623827bdb6ba88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52de0568f86a48daba10ce23f11a0ff1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"548a044b927e42c19f8130ec28ae060b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7691d8792f7d4057ab9168c858782a3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"898218e4362342fe883b9e460f41ba96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b1eaecc4473e4ff3bb41a0218daa478a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4eff220e9bf4df9997aff6d41cfa8ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5674991582e44b898a943aef47be6dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e2edcc6da57042b9900e43fa7a2a048a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7695f2ffa00496a9ce44b37f8fd2ca0","placeholder":"​","style":"IPY_MODEL_4b71d65e426d4caf80623827bdb6ba88","value":" 0/88 [00:00&lt;?, ?it/s]"}},"e7bb7ae5bd7546b99839e44eecc26063":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efc78731443742199f7bdfa0d2ebda87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f30a14d421c2415c9c67827d199508a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a70c7dffa4a4242945ea9f5ab9c20f9","placeholder":"​","style":"IPY_MODEL_548a044b927e42c19f8130ec28ae060b","value":" 2/2 [00:08&lt;00:00,  4.05s/it]"}},"f7695f2ffa00496a9ce44b37f8fd2ca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
