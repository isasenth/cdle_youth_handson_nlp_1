{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21015,"status":"ok","timestamp":1651904298950,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"W6SYl3RTsSih","outputId":"a8312319-5de7-4220-e104-07579814e771"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# !pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1\n","\n","import random\n","import glob\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KQw-Np5brL24"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","train = pd.read_csv(\"data/train.csv\")\n","test = pd.read_csv(\"data/test.csv\")\n","\n","submit = pd.read_csv(\"data/sample_submission.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"unmbc0kxsKkN"},"outputs":[],"source":["def make_dataset(df, max_length, tokenizer):\n","  dataset = []\n","  for index, row in df.iterrows():\n","    encoding = tokenizer(\n","        row['text'],\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True\n","    )\n","    encoding['labels']=row['isFake']\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset.append(encoding)\n","  return dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KO4S6us_vMJm"},"outputs":[],"source":["max_length = 128\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","dataset_for_loader = make_dataset(train, max_length, tokenizer)\n","random.shuffle(dataset_for_loader)\n","n = len(dataset_for_loader)\n","n_train = int(0.7*n)\n","dataset_train = dataset_for_loader[:n_train]\n","dataset_val = dataset_for_loader[n_train:]\n","# dataset_test = make_dataset(test, max_length, tokenizer)\n","\n","dataloader_train = DataLoader(\n","    dataset_train, batch_size=1, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","# dataloader_test = DataLoader(dataset_test, batch_size=256)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VUVgf7B2wkzt"},"outputs":[],"source":["class BertForSequenceClassification_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, num_labels, lr):\n","        super().__init__()\n","        self.save_hyperparameters() \n","        self.bert_sc = BertForSequenceClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","        self.outputs = []\n","        \n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","        \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","\n","    def test_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        labels_predicted = output.logits.argmax(-1)\n","        self.outputs.extend(labels_predicted.tolist())\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1651904318700,"user":{"displayName":"Ryo Yanagimoto","userId":"05411792074620813604"},"user_tz":-540},"id":"It36GYKZx8t-","outputId":"fcf20df6-1c60-4187-b4ea-097dc5fe3022"},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=10,\n","    callbacks = [checkpoint]\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":652,"referenced_widgets":["4265856684614ed7b6066b8b7c40a606","c27a1266ad2746c08391618c9276dd45","bbf87d9db01e4721a5b4f2b6c09ba4bf","d07a18a98b834d1f95c9964ec9bbfda9","11abd6603a494c9e9823ac248e7a59fe","c6f0228c87b340babe10698bb8f2949b","6d490e1a604440fe8db4d9855e337e6e","e78431e4fd0f42d19ccb63e3713c0fd5","76104edb8e7e4d08bfda370a278690a4","d19b2b352165471981ec3b2335d91fbc","ca20faa1a72643b8a0c2253172d2496e","5d25614777634d4096da8dea759efb6d"]},"id":"kMTSvv6dyGau","outputId":"3a50cbf7-7783-47e6-ff19-2dda54d6f16c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type                          | Params\n","----------------------------------------------------------\n","0 | bert_sc | BertForSequenceClassification | 110 M \n","----------------------------------------------------------\n","110 M     Trainable params\n","0         Non-trainable params\n","110 M     Total params\n","442.476   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           \r"]},{"name":"stderr","output_type":"stream","text":["/home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: 100%|█████████▉| 2646/2651 [08:03<00:00,  5.48it/s, loss=0.437, v_num=1] "]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.80 GiB total capacity; 2.03 GiB already allocated; 137.94 MiB free; 2.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000006?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification_pl(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000006?line=1'>2</a>\u001b[0m     MODEL_NAME, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000006?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000006?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dataloader_train, dataloader_val)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=804'>805</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1348'>1349</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1349'>1350</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1350'>1351</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:268\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=263'>264</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:255\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=252'>253</a>\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=253'>254</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=254'>255</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=257'>258</a>\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:309\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=305'>306</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=307'>308</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=308'>309</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=125'>126</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=126'>127</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=127'>128</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=129'>130</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=219'>220</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=220'>221</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=221'>222</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=223'>224</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1759'>1760</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1761'>1762</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1762'>1763</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1764'>1765</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1765'>1766</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=338'>339</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=339'>340</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=340'>341</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=341'>342</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=342'>343</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb Cell 5'\u001b[0m in \u001b[0;36mBertForSequenceClassification_pl.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000004?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000004?line=18'>19</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_sc(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000004?line=19'>20</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryoyanagimoto/cdle_youth_handson_nlp_1/r-yanagimoto/BERT.ipynb#ch0000004?line=20'>21</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, val_loss)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1554\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1545'>1546</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1546'>1547</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1547'>1548</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1548'>1549</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1549'>1550</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1550'>1551</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1551'>1552</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1553'>1554</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1554'>1555</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1555'>1556</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1556'>1557</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1557'>1558</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1558'>1559</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1559'>1560</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1560'>1561</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1561'>1562</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1562'>1563</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1563'>1564</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1565'>1566</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1567'>1568</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1017\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1009'>1010</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1010'>1011</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1011'>1012</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1014'>1015</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1015'>1016</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1016'>1017</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1017'>1018</a>\u001b[0m     embedding_output,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1018'>1019</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1019'>1020</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1020'>1021</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1021'>1022</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1022'>1023</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1023'>1024</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1024'>1025</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1025'>1026</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1026'>1027</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1027'>1028</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1028'>1029</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1029'>1030</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=596'>597</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=597'>598</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=598'>599</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=602'>603</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=603'>604</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=604'>605</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=605'>606</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=606'>607</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=607'>608</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=608'>609</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=609'>610</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=610'>611</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=611'>612</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=612'>613</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=613'>614</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=615'>616</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=616'>617</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=480'>481</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=481'>482</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=482'>483</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=489'>490</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=490'>491</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=491'>492</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=492'>493</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=493'>494</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=494'>495</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=495'>496</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=496'>497</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=497'>498</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=498'>499</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=499'>500</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=501'>502</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=412'>413</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=413'>414</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=414'>415</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=420'>421</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=421'>422</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=422'>423</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=423'>424</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=427'>428</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=428'>429</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=429'>430</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=430'>431</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=431'>432</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=432'>433</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:327\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=323'>324</a>\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=325'>326</a>\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=326'>327</a>\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=328'>329</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ryoyanagimoto/anaconda3/envs/hazumi1902/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=329'>330</a>\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.80 GiB total capacity; 2.03 GiB already allocated; 137.94 MiB free; 2.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["model = BertForSequenceClassification_pl(\n","    MODEL_NAME, num_labels=2, lr=1e-5\n",")\n","\n","trainer.fit(model, dataloader_train, dataloader_val) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jiAWJqFpyRGb","outputId":"78e1f740-800e-4018-e20b-9044917ebab6"},"outputs":[{"name":"stdout","output_type":"stream","text":["ベストモデルのファイル:  /content/drive/MyDrive/FakeNewsDetection/model/epoch=7-step=664.ckpt\n","ベストモデルの検証データに対する損失:  tensor(0.0718, device='cuda:0')\n"]}],"source":["best_model_path = checkpoint.best_model_path\n","print('ベストモデルのファイル: ', checkpoint.best_model_path)\n","print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NuZF0m9t0dKQ"},"outputs":[],"source":["def make_dataset_test(df, max_length, tokenizer):\n","  dataset = []\n","  for index, row in df.iterrows():\n","    encoding = tokenizer(\n","        row['text'],\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True\n","    )\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset.append(encoding)\n","  return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVM6WD18yZA-"},"outputs":[],"source":["dataset_test = make_dataset_test(test, max_length, tokenizer)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)\n","trainer.test(dataloaders=dataloader_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUrG4m0p7dwZ"},"outputs":[],"source":["submit['isFake'] = model.outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsDZHO5wAZaH"},"outputs":[],"source":["submit.to_csv('/content/drive/MyDrive/FakeNewsDetection/submit.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1-Xt9ytGnSW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN4fFx/uicnHlqtNOmkdS87","collapsed_sections":[],"mount_file_id":"1A7sL5FlRoh26XnKfhcwIsb2Ifz_C08YF","name":"FakeNewsDetection.ipynb","provenance":[]},"kernelspec":{"display_name":"hazumi1902","language":"python","name":"hazumi1902"}},"nbformat":4,"nbformat_minor":0}
